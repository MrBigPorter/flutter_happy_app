import 'package:audio_session/audio_session.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart';
import 'package:flutter/material.dart';

class MediaManager {
  MediaStream? localStream;

  //  æŠ¤ç›¾çŠ¶æ€ï¼šè®°ä½å½“å‰æ˜¯è§†é¢‘è¿˜æ˜¯è¯­éŸ³ï¼Œæ–¹ä¾¿æ‹”æ‰è€³æœºæ—¶æ¢å¤
  bool _isVideoMode = true;

  //  æ¿€æ´»éŸ³é¢‘ç„¦ç‚¹ä¸é˜²æ‰“æ–­æŠ¤ç›¾
  Future<void> configureAudioSession(
      bool isVideo,
      bool Function() getIsMuted,
      ) async {
    final session = await AudioSession.instance;
    await session.configure(
      AudioSessionConfiguration(
        avAudioSessionCategory: AVAudioSessionCategory.playAndRecord,
        avAudioSessionCategoryOptions:
        AVAudioSessionCategoryOptions.allowBluetooth |
        (isVideo
            ? AVAudioSessionCategoryOptions.defaultToSpeaker
            : AVAudioSessionCategoryOptions.none),
        avAudioSessionMode: AVAudioSessionMode.voiceChat,
        avAudioSessionRouteSharingPolicy:
        AVAudioSessionRouteSharingPolicy.defaultPolicy,
        avAudioSessionSetActiveOptions: AVAudioSessionSetActiveOptions.none,
        androidAudioAttributes: const AndroidAudioAttributes(
          contentType: AndroidAudioContentType.speech,
          flags: AndroidAudioFlags.none,
          usage: AndroidAudioUsage.voiceCommunication,
        ),
        androidAudioFocusGainType: AndroidAudioFocusGainType.gain,
        androidWillPauseWhenDucked: true,
      ),
    );

    // ç›‘å¬ç³»ç»Ÿçº§æ‰“æ–­ï¼ˆå¦‚æ™®é€šç”µè¯å‘¼å…¥ï¼‰
    session.interruptionEventStream.listen((event) {
      if (event.begin) {
        debugPrint("â˜ï¸ [MediaManager] éŸ³é¢‘ç„¦ç‚¹è¢«æŠ¢å ï¼Œæ‰§è¡Œè¢«åŠ¨é—­éº¦");
        _setMicrophoneEnabled(false);
      } else {
        debugPrint("âœ… [MediaManager] éŸ³é¢‘ç„¦ç‚¹æ¢å¤");
        _setMicrophoneEnabled(!getIsMuted());
      }
    });
  }

  void _setMicrophoneEnabled(bool enabled) {
    if (localStream != null && localStream!.getAudioTracks().isNotEmpty) {
      localStream!.getAudioTracks()[0].enabled = enabled;
    }
  }

  //  æŠ“å–æ‘„åƒå¤´å’Œéº¦å…‹é£
  Future<void> initLocalMedia(
      bool isVideo,
      RTCVideoRenderer localRen,
      RTCVideoRenderer remoteRen,
      ) async {
    _isVideoMode = isVideo; // è®°å½•åˆå§‹æ¨¡å¼

    //  ç¡¬ä»¶çƒ­æ’æ‹”é›·è¾¾ï¼šæ—¶åˆ»ç›‘å¬è“ç‰™/æœ‰çº¿è€³æœºçš„ç‰©ç†æ’æ‹”ï¼
    if (!kIsWeb) {
      navigator.mediaDevices.ondevicechange = (event) {
        debugPrint("ğŸ”Œ [MediaManager] å—…æ¢åˆ°éŸ³é¢‘å¤–è®¾ç‰©ç†æ’æ‹”!");
        _autoRouteAudio();
      };

      // å¯åŠ¨æ—¶å…ˆåšä¸€æ¬¡ç¯å¢ƒä¾¦æµ‹ï¼Œå†³å®šå£°éŸ³ä»å“ªå‡º
      await _autoRouteAudio();
    }

    final mediaConstraints = {
      'audio': true,
      'video': isVideo ? {'facingMode': 'user'} : false,
    };

    localStream = await navigator.mediaDevices.getUserMedia(mediaConstraints);

    // æŠ¤ç›¾ï¼šç¡®ä¿ç”»æ¿ä¸€å®šè¢«åˆå§‹åŒ–
    if (localRen.textureId == null) await localRen.initialize();
    if (remoteRen.textureId == null) await remoteRen.initialize();

    localRen.srcObject = localStream;
  }

  //  æ™ºèƒ½éŸ³é¢‘è·¯ç”±å¤§è„‘ï¼šæ ¹æ®å¤–è®¾æƒ…å†µï¼ŒåŠ¨æ€å‰¥å¤º/èµ‹äºˆæ‰¬å£°å™¨æƒåŠ›
  Future<void> _autoRouteAudio() async {
    if (kIsWeb) return;
    try {
      final devices = await navigator.mediaDevices.enumerateDevices();
      bool hasExternalDevice = false;

      // éå†åº•å±‚ç½‘å¡ï¼Œå¯»æ‰¾æœ‰æ²¡æœ‰æˆ´ä¸Šè“ç‰™æˆ–æ’äº†çº¿
      for (var device in devices) {
        if (device.kind == 'audiooutput' || device.kind == 'audioinput') {
          final label = device.label.toLowerCase();
          if (label.contains('bluetooth') ||
              label.contains('headset') ||
              label.contains('wired')) {
            hasExternalDevice = true;
            break;
          }
        }
      }

      if (hasExternalDevice) {
        debugPrint("ğŸ§ [MediaManager] æ£€æµ‹åˆ°å¤–è®¾æ¥å…¥ï¼Œå¼ºè¡Œå…³é—­æ‰¬å£°å™¨ç‹¬è£ï¼Œå£°éŸ³äº¤è¿˜ç»™è€³æœº");
        // æ ¸å¿ƒå¯†ç ï¼šè®¾ä¸º falseï¼ŒWebRTC å°±ä¼šè‡ªåŠ¨æŠŠå£°éŸ³èµ° SCO è“ç‰™é€šé“
        await Helper.setSpeakerphoneOn(false);
      } else {
        debugPrint("ğŸ“± [MediaManager] æ— å¤–è®¾æ¥å…¥ï¼Œæ¢å¤é»˜è®¤è·¯ç”± (è§†é¢‘:å¤–æ”¾, è¯­éŸ³:å¬ç­’)");
        // æ‹”ä¸‹è€³æœºï¼Œæ¢å¤åŸæ¥çš„è§„çŸ©
        await Helper.setSpeakerphoneOn(_isVideoMode);
      }
    } catch (e) {
      debugPrint("âŒ [MediaManager] è‡ªåŠ¨è·¯ç”±å¤±è´¥: $e");
    }
  }

  void toggleMute(bool isMuted) {
    _setMicrophoneEnabled(!isMuted);
  }

  void toggleCamera(bool isCameraOff) {
    if (localStream != null && localStream!.getVideoTracks().isNotEmpty) {
      localStream!.getVideoTracks()[0].enabled = !isCameraOff;
    }
  }

  Future<void> toggleSpeaker(bool isSpeakerOn) async {
    if (kIsWeb) return;
    try {
      await Helper.setSpeakerphoneOn(isSpeakerOn);
    } catch (_) {}
  }

  void handleAppLifecycleState(AppLifecycleState appState, bool isCameraOff) {
    if (localStream == null) return;
    final videoTracks = localStream!.getVideoTracks();
    if (videoTracks.isEmpty) return;

    if (appState == AppLifecycleState.paused ||
        appState == AppLifecycleState.hidden) {
      videoTracks[0].enabled = false;
    } else if (appState == AppLifecycleState.resumed) {
      if (!isCameraOff) videoTracks[0].enabled = true;
    }
  }

  Future<void> dispose() async {
    //  æ‹”é™¤ç›‘å¬é›·è¾¾ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
    if (!kIsWeb) {
      navigator.mediaDevices.ondevicechange = null;
    }
    localStream?.getTracks().forEach((track) => track.stop());
    await localStream?.dispose();
    localStream = null;
  }
}